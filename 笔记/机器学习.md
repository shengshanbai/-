<!-- TOC -->

- [1. Haar特征](#1-haar特征)
- [2. 决策树](#2-决策树)
    - [2.1. ID3算法](#21-id3算法)
        - [2.1.1. ，信息熵(Entropy)](#211-信息熵entropy)
        - [2.1.2. 、信息增益(Information gain)](#212-信息增益information-gain)
    - [2.2. C4.5算法](#22-c45算法)
    - [2.3. CART算法](#23-cart算法)
        - [2.3.1. **连续值的处理**](#231-连续值的处理)
        - [2.3.2. **CART算法的回归问题**](#232-cart算法的回归问题)
- [决策树减枝](#决策树减枝)

<!-- /TOC -->
# 1. Haar特征
Haar特征最先由Paul Viola等人提出，后经过Rainer Lienhart等扩展引入45°倾斜特征。Haar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。它的计算是白色区域像素和减去黑色区域的像素和。总共分为以下几类：
![HaarTypes](images/HaarTypes.png)
Haar特征总个数的计算:
假设检测窗口大小为W * H，矩形特征大小为w*h，X和Y为表示矩形特征在水平和垂直方向的能放大的最大比例系数：
![HaarCount](images/HaarCount.png)
则总共可以获得的子特征数目为：
![HaarTotal](images/HaarTotal.png)
# 2. 决策树
决策树（decision tree）是一个树结构（**可以是二叉树或非二叉树**）。

其每个非叶节点表示一个**特征属性**上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个**类别**。
![decisonTree](images/decisonTree.png)

构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：

      1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。

      2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。

      3、属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。

属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的贪心策略。这里介绍ID3和C4.5两种常用算法。
## 2.1. ID3算法
ID3算法是由Quinlan首先提出的，该算法是以信息论为基础，以信息熵和信息增益为衡量标准，从而实现对数据的归纳分类。
### 2.1.1. ，信息熵(Entropy)
熵的概念主要是指信息的混乱程度，变量的不确定性越大，熵的值也就越大，熵的公式可以表示为：

$$
Entropy(S)=-\sum_{i=1}^{m}p(u_i)log_2u_i
$$

其中,$p(u_i)=\frac{|u_i|}{|S|}$,表示类别$u_i$在S中出现的概率。

### 2.1.2. 、信息增益(Information gain)
信息增益指的是划分前后熵的变化，可以用下面的公式表示：

$$
infoGain(S,A)=Entropy(S)-\sum_{V \in Value(A)}\frac{|S_V|}{|S|}Entropy(S_V)
$$

其中，$A$表示样本的属性，$Value(A)$是属性$A$所有的取值集合。$V$是$A$的其中一个属性值，$S_V$是$S$中$A$的值为$V$的样例集合。

ID3算法，计算所有属性划分的信息增益，然后选择信息增益最大的属性，上面为了简便，将特征属性离散化了。对于特征属性为连续值，可以如此使用ID3算法：

   先将S中元素按照特征属性排序，则每两个相邻元素的中间点可以看做潜在分裂点，从第一个潜在分裂点开始，分裂S并计算两个集合的期望信息，具有最小期望信息的点称为这个属性的最佳分裂点，其信息期望作为此属性的信息期望。
   
## 2.2. C4.5算法
这次我们每次进行选取特征属性的时候，不再使用ID3算法的信息增益，**而是使用了信息增益率这个概念。**

首先我们来看信息增益率的公式：

$$
Gain\_ratio(D,A)=\frac{Gain(D,A)}{IV(A)}
$$

**由上图我们可以看出，信息增益率=信息增益/IV(A),说明信息增益率是信息增益除了一个属性A的固有值得来的。**

原因是,信息增益准则其实是对可取值数目较多的属性有所偏好！（**比如上面提到的编号，可能取值是实例个数，最多了，分的类别越多，分到每一个子结点，子结点的纯度也就越可能大，因为数量少了嘛，可能在一个类的可能性就最大**）。
我们来看IV(a)的公式：

属性a的固有值：

$$
IV(A)=-\sum_{v=1}^{V}\frac{|D_v|}{|D|}log_2\frac{|D_v|}{|D|}
$$

由上面的计算例子，可以看出IV(A)其实能够反映出，当选取该属性，分成的V类别数越大，IV(A)就越大，如果仅仅只用信息增益来选择属性的话，那么我们偏向于选择分成子节点类别大的那个特征。

于是C4.5算法不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性（_**这样保证了大部分好的的特征**_），再从中选择增益率最高的。

## 2.3. CART算法
CART(Classification And Regression Tree)，分类与回归树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。
分类问题中，假设有K个类别，第k个类别的概率为pk。则基尼系数的表达式为：

$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
$$

CART分类树算法每次仅仅对某个特征的值进行二分，**建立的是二叉树，运算效率更快**，并且基尼系数是**平方计算，比对数计算更快。**

### 2.3.1. **连续值的处理**
同样划分，唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。

**CART分类树对于多离散值的处理问题**，采用的思路是不停的二分离散特征，类似多分类问题，比如某特征有A1.A2.A3三种取值，则CART可能分成A1,(A2,A3)，然后A2,A3在下一层继续二分，总之，CART树是一个二叉树模型。

### 2.3.2. **CART算法的回归问题**
CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。

$$
D=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),\ldots(x_n,y_n)\}
$$

**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。

$$min_{j,s}[min_{c_1}\sum_{x \in R_{1(j,s)}}(y_i-c_1)^2+min_{c_2}\sum_{x \in R_{2(j,s)}}(y_i-c_2)^2]$$

其中$R_m$是被划分的输入空间，$c_m$是空间$R_m$对应的固定输出值。
$$
R_1(j,s)=\{x|x^{(j)} \leq s\} , R_2(j,s)=\{x|x^{(j)} > s\} \\
c_m=\frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i
$$
继续对两个子区域重复前面的步骤，直至满足停止条件。
将输入空间划分为M个区域R1,R2,…,RM，生成决策树：

$$f(x)=\sum_{m=1}^{M}c_mI(x \in R)$$

# 决策树减枝
# Soft cascade结构
为改进VJ框架的缺点而提出，总结的VJ框架缺点:
1. 后一阶段的stage没法利用前一阶段stage的信息。
2. 对于每一级stage，要达到预定的检测率和误报率，越到后面，需要越多的样本和弱分类器。
3. 每一个stage的目标参数，对于检测速度的影响是间接的，如果平衡检测速度和精度，只能不断微调参数，然后重新训练，很浪费时间。
4. 和3类似，其它的自由变量,stage数量等，都和检测速度没有明显关系。

## 算法构想
类似VJ框架，但是我们不使用多个级联的stage,而是用单个的stage，它由很长的T个弱分类器构成，这个分类器的输出定义为：

$$
H(x)=\sum_{t=1}^{T}c_t(x)
$$
其中$c_t(x)=\alpha_th_t(x)$,和VJ框架中每个弱分类器的输出相同。我们根据feature数量的多少，绘制出$H(x)$的函数图如下:
![softHx](images/softHx.png)
可见，随着feature的增加，这个分类器是可以区分开人脸和非人脸的。VJ框架是预设了一个阈值$r_t$,如果当前阶段的$H(x)$值大于$r_t$的人脸比例大于检测率，而非人脸大于$r_t$的比例小于误检率，就结束了这个stage。

知道这一原理后，我们可以替换为以下过程。对于每一个弱分类器设定一个阈值$r_t$,在计算完输出后，如果累计值$H(x)$小于阈值，我们就输出非人脸，否则继续计算。如果通过了所有的T个分类器，这就是人脸图片。

# ACF特征
目的：为了解决多角度人脸问题。
channel feature:将原始图片映射为梯度图或者方向梯度直方图，再在这些图的基础上提取特征。一个channel表示一个这样的映射关系。
ACF特征的计算流程大致如下:
![ACF](images/ACF.png)
分类方法：1，将VJ框架中的只有根节点的决策树，改为深度为2的决策树。2，使用soft cascade结构取代VJ框架中的多个级联stage。
