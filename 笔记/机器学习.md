<!-- TOC -->

- [1. Haar特征](#1-haar特征)
- [2. 决策树](#2-决策树)
    - [2.1. ID3算法](#21-id3算法)
        - [2.1.1. ，信息熵(Entropy)](#211-信息熵entropy)
        - [2.1.2. 、信息增益(Information gain)](#212-信息增益information-gain)
    - [2.2. C4.5算法](#22-c45算法)
    - [2.3. CART算法](#23-cart算法)
        - [2.3.1. **连续值的处理**](#231-连续值的处理)
        - [2.3.2. **CART算法的回归问题**](#232-cart算法的回归问题)
- [决策树减枝](#决策树减枝)

<!-- /TOC -->
# 1. Haar特征
Haar特征最先由Paul Viola等人提出，后经过Rainer Lienhart等扩展引入45°倾斜特征。Haar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。它的计算是白色区域像素和减去黑色区域的像素和。总共分为以下几类：
![HaarTypes](images/HaarTypes.png)
Haar特征总个数的计算:
假设检测窗口大小为W * H，矩形特征大小为w*h，X和Y为表示矩形特征在水平和垂直方向的能放大的最大比例系数：
![HaarCount](images/HaarCount.png)
则总共可以获得的子特征数目为：
![HaarTotal](images/HaarTotal.png)
# 2. 决策树
决策树（decision tree）是一个树结构（**可以是二叉树或非二叉树**）。

其每个非叶节点表示一个**特征属性**上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个**类别**。
![decisonTree](images/decisonTree.png)

构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：

      1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。

      2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。

      3、属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。

属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的贪心策略。这里介绍ID3和C4.5两种常用算法。
## 2.1. ID3算法
ID3算法是由Quinlan首先提出的，该算法是以信息论为基础，以信息熵和信息增益为衡量标准，从而实现对数据的归纳分类。
### 2.1.1. ，信息熵(Entropy)
熵的概念主要是指信息的混乱程度，变量的不确定性越大，熵的值也就越大，熵的公式可以表示为：

$$
Entropy(S)=-\sum_{i=1}^{m}p(u_i)log_2u_i
$$

其中,$p(u_i)=\frac{|u_i|}{|S|}$,表示类别$u_i$在S中出现的概率。

### 2.1.2. 、信息增益(Information gain)
信息增益指的是划分前后熵的变化，可以用下面的公式表示：

$$
infoGain(S,A)=Entropy(S)-\sum_{V \in Value(A)}\frac{|S_V|}{|S|}Entropy(S_V)
$$

其中，$A$表示样本的属性，$Value(A)$是属性$A$所有的取值集合。$V$是$A$的其中一个属性值，$S_V$是$S$中$A$的值为$V$的样例集合。

ID3算法，计算所有属性划分的信息增益，然后选择信息增益最大的属性，上面为了简便，将特征属性离散化了。对于特征属性为连续值，可以如此使用ID3算法：

   先将S中元素按照特征属性排序，则每两个相邻元素的中间点可以看做潜在分裂点，从第一个潜在分裂点开始，分裂S并计算两个集合的期望信息，具有最小期望信息的点称为这个属性的最佳分裂点，其信息期望作为此属性的信息期望。
   
## 2.2. C4.5算法
这次我们每次进行选取特征属性的时候，不再使用ID3算法的信息增益，**而是使用了信息增益率这个概念。**

首先我们来看信息增益率的公式：

$$
Gain\_ratio(D,A)=\frac{Gain(D,A)}{IV(A)}
$$

**由上图我们可以看出，信息增益率=信息增益/IV(A),说明信息增益率是信息增益除了一个属性A的固有值得来的。**

原因是,信息增益准则其实是对可取值数目较多的属性有所偏好！（**比如上面提到的编号，可能取值是实例个数，最多了，分的类别越多，分到每一个子结点，子结点的纯度也就越可能大，因为数量少了嘛，可能在一个类的可能性就最大**）。
我们来看IV(a)的公式：

属性a的固有值：

$$
IV(A)=-\sum_{v=1}^{V}\frac{|D_v|}{|D|}log_2\frac{|D_v|}{|D|}
$$

由上面的计算例子，可以看出IV(A)其实能够反映出，当选取该属性，分成的V类别数越大，IV(A)就越大，如果仅仅只用信息增益来选择属性的话，那么我们偏向于选择分成子节点类别大的那个特征。

于是C4.5算法不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性（_**这样保证了大部分好的的特征**_），再从中选择增益率最高的。

## 2.3. CART算法
CART(Classification And Regression Tree)，分类与回归树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。
分类问题中，假设有K个类别，第k个类别的概率为pk。则基尼系数的表达式为：

$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
$$

CART分类树算法每次仅仅对某个特征的值进行二分，**建立的是二叉树，运算效率更快**，并且基尼系数是**平方计算，比对数计算更快。**

### 2.3.1. **连续值的处理**
同样划分，唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。

**CART分类树对于多离散值的处理问题**，采用的思路是不停的二分离散特征，类似多分类问题，比如某特征有A1.A2.A3三种取值，则CART可能分成A1,(A2,A3)，然后A2,A3在下一层继续二分，总之，CART树是一个二叉树模型。

### 2.3.2. **CART算法的回归问题**
CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。

$$
D=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),\ldots(x_n,y_n)\}
$$

**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。

$$min_{j,s}[min_{c_1}\sum_{x \in R_{1(j,s)}}(y_i-c_1)^2+min_{c_2}\sum_{x \in R_{2(j,s)}}(y_i-c_2)^2]$$

其中$R_m$是被划分的输入空间，$c_m$是空间$R_m$对应的固定输出值。
$$
R_1(j,s)=\{x|x^{(j)} \leq s\} , R_2(j,s)=\{x|x^{(j)} > s\} \\
c_m=\frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i
$$
继续对两个子区域重复前面的步骤，直至满足停止条件。
将输入空间划分为M个区域R1,R2,…,RM，生成决策树：

$$f(x)=\sum_{m=1}^{M}c_mI(x \in R)$$

# 决策树减枝